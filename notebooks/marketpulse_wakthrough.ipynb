{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MarketPulse Analytics Studio - Complete Walkthrough\n",
    "## Cazandra Aporbo, MS\n",
    "## May-Sep 2025\n",
    "\n",
    "This notebook walks through my entire MarketPulse system. I'll show you how sentiment analysis, feature engineering, and ensemble learning come together to predict market movements. \n",
    "\n",
    "I wrote this notebook to be readable by humans. Every variable name tells you what it does. Every comment explains why, not just what. By the end, you'll understand how to build a trading system that actually works.\n",
    "\n",
    "Let's dive in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I always start with imports organized by purpose\n",
    "# Makes it easier to see what the code actually does\n",
    "\n",
    "# Data manipulation - the foundation of everything\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Visualization - because numbers without pictures are just numbers\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning - where the magic happens\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# My custom modules - this is where I put months of work\n",
    "import sys\n",
    "sys.path.append('../')  # Add parent directory to path\n",
    "\n",
    "from core.sentiment_engine import SentimentEngine, FinancialLexicon\n",
    "from core.feature_factory import FeatureFactory, FeatureConfig\n",
    "from core.model_ensemble import ModelEnsemble, EnsembleConfig\n",
    "\n",
    "# Styling for prettier outputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(\"Everything loaded. Let's build something cool.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Market Data\n",
    "\n",
    "I'm using synthetic data here so anyone can run this notebook without API keys. In production, you'd pull from Yahoo Finance or your broker's API. The synthetic data mimics real market behavior - trends, volatility clusters, the works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_realistic_market_data(ticker='DEMO', days=365):\n",
    "    \"\"\"\n",
    "    I create synthetic market data that actually looks like real markets.\n",
    "    Random walks are for academics. Real markets trend, mean-revert, and cluster.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Start from a year ago\n",
    "    dates = pd.date_range(end=datetime.now(), periods=days, freq='D')\n",
    "    \n",
    "    # I use a combination of trends and noise to make it realistic\n",
    "    trend_strength = 0.0003  # Slight upward bias, like real markets\n",
    "    volatility = 0.02  # 2% daily vol is about right for stocks\n",
    "    \n",
    "    # Generate returns with volatility clustering\n",
    "    # Markets go through calm and stormy periods\n",
    "    daily_returns = []\n",
    "    current_vol = volatility\n",
    "    \n",
    "    for i in range(days):\n",
    "        # Volatility clusters - if yesterday was volatile, today probably is too\n",
    "        if i > 0 and abs(daily_returns[-1]) > volatility * 1.5:\n",
    "            current_vol = volatility * 1.5  # Elevated volatility\n",
    "        else:\n",
    "            current_vol = current_vol * 0.9 + volatility * 0.1  # Decay back to normal\n",
    "        \n",
    "        # Generate return with trend and noise\n",
    "        daily_return = np.random.normal(trend_strength, current_vol)\n",
    "        daily_returns.append(daily_return)\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    # I start at 100 because it's a nice round number\n",
    "    initial_price = 100\n",
    "    prices = initial_price * np.exp(np.cumsum(daily_returns))\n",
    "    \n",
    "    # Create OHLC data - the four numbers that define a trading day\n",
    "    # I add some intraday variation to make it realistic\n",
    "    market_data = pd.DataFrame(index=dates)\n",
    "    market_data['Close'] = prices\n",
    "    \n",
    "    # Open is usually close to previous close (gaps happen but not always)\n",
    "    market_data['Open'] = market_data['Close'].shift(1) * np.random.uniform(0.995, 1.005, days)\n",
    "    market_data['Open'].fillna(initial_price, inplace=True)\n",
    "    \n",
    "    # High and Low show the day's range\n",
    "    # Bigger moves mean bigger ranges\n",
    "    daily_range = abs(daily_returns) + 0.005  # Minimum 0.5% range\n",
    "    market_data['High'] = market_data[['Open', 'Close']].max(axis=1) * (1 + daily_range)\n",
    "    market_data['Low'] = market_data[['Open', 'Close']].min(axis=1) * (1 - daily_range)\n",
    "    \n",
    "    # Volume - higher on big move days (people trade more when stuff happens)\n",
    "    base_volume = 10_000_000  # 10 million shares baseline\n",
    "    volume_multiplier = 1 + abs(daily_returns) * 10  # Big moves = big volume\n",
    "    market_data['Volume'] = (base_volume * volume_multiplier * \n",
    "                             np.random.uniform(0.8, 1.2, days)).astype(int)\n",
    "    \n",
    "    # Add the ticker for reference\n",
    "    market_data['Ticker'] = ticker\n",
    "    \n",
    "    return market_data\n",
    "\n",
    "# Generate our test data\n",
    "market_data = create_realistic_market_data(ticker='TECH', days=500)\n",
    "\n",
    "print(f\"Created {len(market_data)} days of market data\")\n",
    "print(f\"Price range: ${market_data['Close'].min():.2f} to ${market_data['Close'].max():.2f}\")\n",
    "print(f\"\\nLast 5 days:\")\n",
    "market_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate News Sentiment\n",
    "\n",
    "Real markets react to news. I'll create synthetic news that correlates with price moves, but not perfectly. Sometimes good news is ignored. Sometimes rumors move markets more than facts. That's the reality I'm modeling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_market_news(market_data, news_per_day=3):\n",
    "    \"\"\"\n",
    "    I generate synthetic news that somewhat correlates with price moves.\n",
    "    The correlation isn't perfect because markets aren't efficient.\n",
    "    Sometimes the news follows price, sometimes price follows news.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate daily returns to base sentiment on\n",
    "    returns = market_data['Close'].pct_change()\n",
    "    \n",
    "    # News headlines that sound real\n",
    "    # I spent way too long making these sound authentic\n",
    "    bullish_templates = [\n",
    "        \"TECH beats earnings expectations by {:.1%}, raises full-year guidance\",\n",
    "        \"Analysts upgrade TECH to buy, cite strong fundamentals\",\n",
    "        \"TECH announces breakthrough product, stock rallies\",\n",
    "        \"Institutional investors increase TECH holdings by {:.0f}%\",\n",
    "        \"TECH CEO discusses expansion plans, market reacts positively\",\n",
    "        \"Breaking: TECH wins major contract worth ${:.0f}M\",\n",
    "        \"TECH stock hits new 52-week high on strong volume\"\n",
    "    ]\n",
    "    \n",
    "    bearish_templates = [\n",
    "        \"TECH misses revenue estimates, shares tumble {:.1%}\",\n",
    "        \"Concerns grow over TECH valuation metrics\",\n",
    "        \"TECH faces regulatory scrutiny, uncertainty weighs on stock\",\n",
    "        \"Major investor reduces TECH position by {:.0f}%\",\n",
    "        \"TECH announces layoffs, restructuring costs mount\",\n",
    "        \"Supply chain issues impact TECH production forecasts\",\n",
    "        \"TECH loses market share to competitors, analysts concerned\"\n",
    "    ]\n",
    "    \n",
    "    neutral_templates = [\n",
    "        \"TECH trading flat as investors await earnings report\",\n",
    "        \"Market watches TECH for breakout signals\",\n",
    "        \"TECH maintains steady course amid market volatility\",\n",
    "        \"Analysts mixed on TECH near-term prospects\",\n",
    "        \"TECH consolidating after recent moves\"\n",
    "    ]\n",
    "    \n",
    "    # Sources with different reliability\n",
    "    reliable_sources = ['Bloomberg', 'Reuters', 'WSJ', 'Financial Times']\n",
    "    medium_sources = ['MarketWatch', 'CNBC', 'Yahoo Finance']\n",
    "    noise_sources = ['TradingBlog', 'StockTwits', 'Reddit']\n",
    "    \n",
    "    all_news = []\n",
    "    \n",
    "    for date, row in market_data.iterrows():\n",
    "        daily_return = returns.loc[date]\n",
    "        \n",
    "        # Generate multiple news items per day\n",
    "        for news_item in range(news_per_day):\n",
    "            # News sentiment loosely follows price\n",
    "            # But I add noise because markets aren't perfectly efficient\n",
    "            sentiment_bias = daily_return * 20  # Scale return to sentiment\n",
    "            sentiment_noise = np.random.normal(0, 0.3)  # Random noise\n",
    "            \n",
    "            # Final sentiment with some randomness\n",
    "            true_sentiment = np.clip(sentiment_bias + sentiment_noise, -1, 1)\n",
    "            \n",
    "            # Pick headline based on sentiment\n",
    "            if true_sentiment > 0.2:\n",
    "                headline = np.random.choice(bullish_templates)\n",
    "                # Format with random numbers for realism\n",
    "                headline = headline.format(abs(daily_return) * np.random.uniform(1, 3))\n",
    "            elif true_sentiment < -0.2:\n",
    "                headline = np.random.choice(bearish_templates)\n",
    "                headline = headline.format(abs(daily_return) * np.random.uniform(1, 3))\n",
    "            else:\n",
    "                headline = np.random.choice(neutral_templates)\n",
    "            \n",
    "            # Pick source based on sentiment magnitude\n",
    "            # Big news comes from reliable sources\n",
    "            if abs(true_sentiment) > 0.5:\n",
    "                source = np.random.choice(reliable_sources)\n",
    "            elif abs(true_sentiment) > 0.2:\n",
    "                source = np.random.choice(medium_sources)\n",
    "            else:\n",
    "                source = np.random.choice(noise_sources)\n",
    "            \n",
    "            # Add some time variation during the day\n",
    "            hours_offset = np.random.randint(0, 24)\n",
    "            timestamp = date + timedelta(hours=hours_offset)\n",
    "            \n",
    "            all_news.append({\n",
    "                'timestamp': timestamp,\n",
    "                'headline': headline,\n",
    "                'source': source,\n",
    "                'true_sentiment': true_sentiment  # I keep this for validation\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(all_news)\n",
    "\n",
    "# Generate news for our market data\n",
    "news_data = generate_market_news(market_data, news_per_day=5)\n",
    "\n",
    "print(f\"Generated {len(news_data)} news articles\")\n",
    "print(f\"Sources: {news_data['source'].value_counts().to_dict()}\")\n",
    "print(f\"\\nRecent headlines:\")\n",
    "for _, article in news_data.tail(5).iterrows():\n",
    "    print(f\"  [{article['source']}] {article['headline'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Sentiment Analysis\n",
    "\n",
    "Now I'll run my sentiment engine on the news. This shows how different sources get weighted differently and how sentiment momentum is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize my sentiment engine\n",
    "sentiment_brain = SentimentEngine()\n",
    "\n",
    "# Process all the news\n",
    "print(\"Analyzing sentiment for all articles...\")\n",
    "analyzed_articles = sentiment_brain.analyze_batch(\n",
    "    news_data.to_dict('records')\n",
    ")\n",
    "\n",
    "print(f\"\\nSentiment Statistics:\")\n",
    "print(f\"  Average sentiment: {analyzed_articles['weighted_score'].mean():.3f}\")\n",
    "print(f\"  Sentiment volatility: {analyzed_articles['weighted_score'].std():.3f}\")\n",
    "print(f\"  Most positive day: {analyzed_articles['weighted_score'].max():.3f}\")\n",
    "print(f\"  Most negative day: {analyzed_articles['weighted_score'].min():.3f}\")\n",
    "\n",
    "# Show how sentiment momentum works\n",
    "# This is the secret sauce - it's not just sentiment, it's how fast it's changing\n",
    "latest_sentiment = analyzed_articles.tail(20)\n",
    "\n",
    "# Visualize sentiment dynamics\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=(\n",
    "        'Raw Sentiment Scores',\n",
    "        'Sentiment Momentum (3-day vs 10-day)',\n",
    "        'Sentiment Acceleration'\n",
    "    ),\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "# Raw sentiment\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=latest_sentiment['timestamp'],\n",
    "        y=latest_sentiment['weighted_score'],\n",
    "        mode='lines+markers',\n",
    "        name='Weighted Sentiment',\n",
    "        line=dict(color='#4a9b7f', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Sentiment momentum\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=latest_sentiment['timestamp'],\n",
    "        y=latest_sentiment['sentiment_momentum'],\n",
    "        mode='lines',\n",
    "        name='Momentum',\n",
    "        line=dict(color='#457b9d', width=2),\n",
    "        fill='tozeroy'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Sentiment acceleration\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=latest_sentiment['timestamp'],\n",
    "        y=latest_sentiment['sentiment_acceleration'],\n",
    "        mode='lines',\n",
    "        name='Acceleration',\n",
    "        line=dict(color='#c0504d', width=2)\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    showlegend=False,\n",
    "    title_text=\"Sentiment Dynamics - The Three Dimensions\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Aggregate sentiment to daily level for merging with market data\n",
    "daily_sentiment = analyzed_articles.groupby(\n",
    "    analyzed_articles['timestamp'].dt.date\n",
    ").agg({\n",
    "    'weighted_score': 'mean',\n",
    "    'confidence': 'mean',\n",
    "    'text': 'count'  # News volume\n",
    "}).rename(columns={'text': 'news_count'})\n",
    "\n",
    "daily_sentiment.index = pd.to_datetime(daily_sentiment.index)\n",
    "\n",
    "print(f\"\\nAggregated to {len(daily_sentiment)} daily sentiment scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering\n",
    "\n",
    "This is where data science becomes art. I combine price, volume, technical indicators, and sentiment into features that actually predict something. Started with 200+ features, kept the ones that matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge market data with sentiment\n",
    "# I need to be careful about alignment here - markets and news run on different clocks\n",
    "combined_data = market_data.copy()\n",
    "combined_data['sentiment'] = daily_sentiment['weighted_score']\n",
    "combined_data['news_count'] = daily_sentiment['news_count']\n",
    "\n",
    "# Forward fill sentiment for days without news (weekends, holidays)\n",
    "combined_data['sentiment'].fillna(method='ffill', inplace=True)\n",
    "combined_data['news_count'].fillna(0, inplace=True)\n",
    "\n",
    "# Initialize my feature factory\n",
    "feature_config = FeatureConfig(\n",
    "    fast_window=5,\n",
    "    medium_window=20,\n",
    "    slow_window=50,\n",
    "    max_features=30  # Keep it manageable\n",
    ")\n",
    "\n",
    "feature_wizard = FeatureFactory(config=feature_config)\n",
    "\n",
    "# Create all features\n",
    "print(\"Engineering features from raw data...\")\n",
    "feature_matrix = feature_wizard.create_features(\n",
    "    combined_data,\n",
    "    include_sentiment=True\n",
    ")\n",
    "\n",
    "print(f\"\\nCreated {len(feature_matrix.columns)} features\")\n",
    "print(f\"Feature categories:\")\n",
    "\n",
    "# I like to see what types of features I have\n",
    "feature_categories = {\n",
    "    'Price': [col for col in feature_matrix.columns if 'price' in col.lower() or 'return' in col.lower()],\n",
    "    'Volume': [col for col in feature_matrix.columns if 'volume' in col.lower()],\n",
    "    'Volatility': [col for col in feature_matrix.columns if 'vol' in col.lower() or 'atr' in col.lower()],\n",
    "    'Momentum': [col for col in feature_matrix.columns if 'rsi' in col or 'roc' in col or 'macd' in col],\n",
    "    'Sentiment': [col for col in feature_matrix.columns if 'sent' in col.lower()],\n",
    "}\n",
    "\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"  {category}: {len(features)} features\")\n",
    "\n",
    "# Show correlation between key features\n",
    "key_features = ['returns', 'sentiment', 'volume_ratio', 'volatility_20', 'rsi']\n",
    "available_features = [f for f in key_features if f in feature_matrix.columns]\n",
    "\n",
    "if len(available_features) > 0:\n",
    "    correlation_matrix = feature_matrix[available_features].corr()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        correlation_matrix,\n",
    "        text_auto=True,\n",
    "        color_continuous_scale='RdBu',\n",
    "        zmin=-1, zmax=1,\n",
    "        title=\"Feature Correlation Matrix - Looking for Relationships\"\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "\n",
    "# Show feature importance (based on variance)\n",
    "feature_importance = feature_wizard.get_feature_importance()\n",
    "if feature_importance:\n",
    "    top_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(\"\\nTop 10 Features by Importance:\")\n",
    "    for rank, (feature, score) in enumerate(top_features, 1):\n",
    "        print(f\"  {rank}. {feature}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Trading Signals\n",
    "\n",
    "I need to create labels for supervised learning. In this case, I'm predicting whether tomorrow's price will be up or down. Simple but effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target variable - next day's direction\n",
    "# I'm predicting tomorrow based on today's features\n",
    "tomorrow_return = combined_data['Close'].pct_change().shift(-1)\n",
    "trading_signal = (tomorrow_return > 0).astype(int)\n",
    "\n",
    "# Remove the last row (no tomorrow for that one)\n",
    "feature_matrix = feature_matrix[:-1]\n",
    "trading_signal = trading_signal[:-1]\n",
    "\n",
    "# Remove any remaining NaN values\n",
    "# I need clean data for the models\n",
    "clean_mask = ~(feature_matrix.isna().any(axis=1) | trading_signal.isna())\n",
    "X_clean = feature_matrix[clean_mask]\n",
    "y_clean = trading_signal[clean_mask]\n",
    "\n",
    "print(f\"Final dataset: {len(X_clean)} samples with {len(X_clean.columns)} features\")\n",
    "print(f\"Target distribution:\")\n",
    "print(f\"  Up days (1): {y_clean.sum()} ({y_clean.mean():.1%})\")\n",
    "print(f\"  Down days (0): {len(y_clean) - y_clean.sum()} ({1-y_clean.mean():.1%})\")\n",
    "\n",
    "# Split data temporally (never randomly for time series!)\n",
    "# I use 80% for training, 20% for testing\n",
    "split_point = int(len(X_clean) * 0.8)\n",
    "\n",
    "X_train = X_clean.iloc[:split_point]\n",
    "X_test = X_clean.iloc[split_point:]\n",
    "y_train = y_clean.iloc[:split_point]\n",
    "y_test = y_clean.iloc[split_point:]\n",
    "\n",
    "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nFeatures look like this:\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Ensemble\n",
    "\n",
    "Now for the fun part - training multiple models and combining them. Each model sees the problem differently, and that diversity is our strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the ensemble\n",
    "# I've tuned these weights based on backtesting\n",
    "ensemble_config = EnsembleConfig(\n",
    "    model_weights={\n",
    "        'logistic': 0.3,     # Simple but stable\n",
    "        'random_forest': 0.7  # Captures non-linearity\n",
    "    },\n",
    "    n_splits=3,  # For time series cross-validation\n",
    "    min_accuracy=0.52  # Better than coin flip\n",
    ")\n",
    "\n",
    "# Initialize the ensemble\n",
    "market_oracle = ModelEnsemble(config=ensemble_config)\n",
    "\n",
    "# Train all models\n",
    "print(\"Training the ensemble (this takes a moment)...\\n\")\n",
    "training_metrics = market_oracle.train(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    validate=True  # Use time series cross-validation\n",
    ")\n",
    "\n",
    "# Display training results\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for model_name, metrics in training_metrics.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    print(f\"  Accuracy:  {metrics.accuracy:.3f}\")\n",
    "    print(f\"  Precision: {metrics.precision:.3f}\")\n",
    "    print(f\"  Recall:    {metrics.recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {metrics.f1:.3f}\")\n",
    "\n",
    "# Get ensemble weights\n",
    "final_weights = market_oracle.get_model_weights()\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENSEMBLE WEIGHTS (Performance-Adjusted)\")\n",
    "print(\"=\"*50)\n",
    "for model, weight in final_weights.items():\n",
    "    print(f\"  {model}: {weight:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions\n",
    "\n",
    "Time to see how well our ensemble performs on data it's never seen. This is the moment of truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "print(\"Making predictions on test set...\\n\")\n",
    "\n",
    "# Get predictions from ensemble\n",
    "ensemble_predictions = market_oracle.predict(X_test, method='weighted')\n",
    "\n",
    "# Also get individual model predictions for comparison\n",
    "individual_predictions = {}\n",
    "for model_name, model in market_oracle.models.items():\n",
    "    individual_predictions[model_name] = model.predict(X_test)\n",
    "\n",
    "# Calculate performance metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Ensemble performance\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"\\nENSEMBLE ACCURACY: {ensemble_accuracy:.3f}\")\n",
    "\n",
    "# Individual model performance\n",
    "print(\"\\nIndividual Model Accuracies:\")\n",
    "for model_name, predictions in individual_predictions.items():\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(f\"  {model_name}: {accuracy:.3f}\")\n",
    "\n",
    "# Detailed classification report for ensemble\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ENSEMBLE CLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    ensemble_predictions,\n",
    "    target_names=['DOWN', 'UP']\n",
    "))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, ensemble_predictions)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(\"                Predicted\")\n",
    "print(\"                DOWN   UP\")\n",
    "print(f\"Actual DOWN     {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "print(f\"       UP       {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "# Calculate some trading metrics\n",
    "# This is what actually matters for making money\n",
    "true_positives = cm[1,1]  # Correctly predicted up days\n",
    "false_positives = cm[0,1]  # Incorrectly predicted up days\n",
    "true_negatives = cm[0,0]  # Correctly predicted down days\n",
    "false_negatives = cm[1,0]  # Incorrectly predicted down days\n",
    "\n",
    "if (true_positives + false_positives) > 0:\n",
    "    precision_up = true_positives / (true_positives + false_positives)\n",
    "    print(f\"\\nWhen we predict UP, we're right {precision_up:.1%} of the time\")\n",
    "\n",
    "if (true_negatives + false_negatives) > 0:\n",
    "    precision_down = true_negatives / (true_negatives + false_negatives)\n",
    "    print(f\"When we predict DOWN, we're right {precision_down:.1%} of the time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Performance\n",
    "\n",
    "Numbers are nice, but pictures tell the story better. Let's see how our predictions look over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a results dataframe for visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'date': X_test.index,\n",
    "    'actual': y_test.values,\n",
    "    'predicted': ensemble_predictions,\n",
    "    'correct': (y_test.values == ensemble_predictions).astype(int)\n",
    "})\n",
    "\n",
    "# Add prices for context\n",
    "results_df['price'] = combined_data.loc[X_test.index, 'Close'].values\n",
    "\n",
    "# Calculate cumulative accuracy over time\n",
    "# I want to see if the model gets better or worse over time\n",
    "results_df['cumulative_accuracy'] = results_df['correct'].expanding().mean()\n",
    "\n",
    "# Create visualization\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=(\n",
    "        'Stock Price with Predictions',\n",
    "        'Prediction Accuracy Over Time',\n",
    "        'Rolling 20-Day Accuracy'\n",
    "    ),\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Plot 1: Price with prediction markers\n",
    "# Green dots for correct predictions, red for wrong ones\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['date'],\n",
    "        y=results_df['price'],\n",
    "        mode='lines',\n",
    "        name='Price',\n",
    "        line=dict(color='#2E86AB', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add correct predictions\n",
    "correct_mask = results_df['correct'] == 1\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df[correct_mask]['date'],\n",
    "        y=results_df[correct_mask]['price'],\n",
    "        mode='markers',\n",
    "        name='Correct',\n",
    "        marker=dict(color='#4a9b7f', size=6)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add wrong predictions\n",
    "wrong_mask = results_df['correct'] == 0\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df[wrong_mask]['date'],\n",
    "        y=results_df[wrong_mask]['price'],\n",
    "        mode='markers',\n",
    "        name='Wrong',\n",
    "        marker=dict(color='#c0504d', size=6)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Plot 2: Cumulative accuracy\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['date'],\n",
    "        y=results_df['cumulative_accuracy'],\n",
    "        mode='lines',\n",
    "        name='Cumulative Accuracy',\n",
    "        line=dict(color='#F18F01', width=2),\n",
    "        fill='tozeroy'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Add 50% reference line (random guessing)\n",
    "fig.add_hline(\n",
    "    y=0.5, \n",
    "    line_dash=\"dash\", \n",
    "    line_color=\"gray\",\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Plot 3: Rolling accuracy\n",
    "results_df['rolling_accuracy'] = results_df['correct'].rolling(20).mean()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['date'],\n",
    "        y=results_df['rolling_accuracy'],\n",
    "        mode='lines',\n",
    "        name='20-Day Rolling',\n",
    "        line=dict(color='#8B5A3C', width=2)\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "# Add 50% reference line\n",
    "fig.add_hline(\n",
    "    y=0.5, \n",
    "    line_dash=\"dash\", \n",
    "    line_color=\"gray\",\n",
    "    row=3, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    title_text=\"Model Performance Analysis - How Did We Do?\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text=\"Price ($)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", tickformat=\".0%\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Accuracy\", tickformat=\".0%\", row=3, col=1)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Overall Accuracy: {results_df['correct'].mean():.1%}\")\n",
    "print(f\"Best 20-day Period: {results_df['rolling_accuracy'].max():.1%}\")\n",
    "print(f\"Worst 20-day Period: {results_df['rolling_accuracy'].min():.1%}\")\n",
    "print(f\"Accuracy Volatility: {results_df['rolling_accuracy'].std():.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Simulate Trading Performance\n",
    "\n",
    "Accuracy is nice, but what really matters is: can this make money? Let's simulate a simple trading strategy based on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_trading(results_df, initial_capital=10000):\n",
    "    \"\"\"\n",
    "    I simulate a simple trading strategy:\n",
    "    - Buy when model predicts UP\n",
    "    - Sell (or stay out) when model predicts DOWN\n",
    "    \n",
    "    This ignores transaction costs for simplicity.\n",
    "    In real trading, those matter a lot.\n",
    "    \"\"\"\n",
    "    \n",
    "    capital = initial_capital\n",
    "    shares = 0\n",
    "    trades = []\n",
    "    portfolio_value = []\n",
    "    \n",
    "    for i, row in results_df.iterrows():\n",
    "        current_price = row['price']\n",
    "        \n",
    "        # Portfolio value = cash + stock value\n",
    "        current_value = capital + (shares * current_price)\n",
    "        portfolio_value.append(current_value)\n",
    "        \n",
    "        # Trading logic\n",
    "        if row['predicted'] == 1 and shares == 0:\n",
    "            # Buy signal and we're not in the market\n",
    "            shares_to_buy = capital // current_price\n",
    "            if shares_to_buy > 0:\n",
    "                shares = shares_to_buy\n",
    "                capital -= shares * current_price\n",
    "                trades.append({\n",
    "                    'date': row['date'],\n",
    "                    'action': 'BUY',\n",
    "                    'price': current_price,\n",
    "                    'shares': shares\n",
    "                })\n",
    "                \n",
    "        elif row['predicted'] == 0 and shares > 0:\n",
    "            # Sell signal and we have position\n",
    "            capital += shares * current_price\n",
    "            trades.append({\n",
    "                'date': row['date'],\n",
    "                'action': 'SELL',\n",
    "                'price': current_price,\n",
    "                'shares': shares\n",
    "            })\n",
    "            shares = 0\n",
    "    \n",
    "    # Close any remaining position\n",
    "    if shares > 0:\n",
    "        final_price = results_df.iloc[-1]['price']\n",
    "        capital += shares * final_price\n",
    "        trades.append({\n",
    "            'date': results_df.iloc[-1]['date'],\n",
    "            'action': 'SELL',\n",
    "            'price': final_price,\n",
    "            'shares': shares\n",
    "        })\n",
    "    \n",
    "    return portfolio_value, trades, capital\n",
    "\n",
    "# Run the simulation\n",
    "portfolio_values, trade_history, final_capital = simulate_trading(results_df)\n",
    "\n",
    "# Calculate buy and hold for comparison\n",
    "# This is our benchmark - could we beat just holding the stock?\n",
    "buy_hold_shares = 10000 // results_df.iloc[0]['price']\n",
    "buy_hold_values = [buy_hold_shares * price for price in results_df['price']]\n",
    "\n",
    "# Calculate returns\n",
    "strategy_return = (final_capital - 10000) / 10000\n",
    "buy_hold_return = (buy_hold_values[-1] - 10000) / 10000\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TRADING SIMULATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nStarting Capital: $10,000\")\n",
    "print(f\"Final Capital (Our Strategy): ${final_capital:,.2f}\")\n",
    "print(f\"Final Value (Buy & Hold): ${buy_hold_values[-1]:,.2f}\")\n",
    "print(f\"\\nReturns:\")\n",
    "print(f\"  Our Strategy: {strategy_return:+.1%}\")\n",
    "print(f\"  Buy & Hold: {buy_hold_return:+.1%}\")\n",
    "print(f\"  Alpha (Outperformance): {strategy_return - buy_hold_return:+.1%}\")\n",
    "\n",
    "print(f\"\\nNumber of Trades: {len(trade_history)}\")\n",
    "if len(trade_history) > 0:\n",
    "    print(f\"\\nSample Trades:\")\n",
    "    for trade in trade_history[:5]:\n",
    "        print(f\"  {trade['date'].date()}: {trade['action']} {trade['shares']} @ ${trade['price']:.2f}\")\n",
    "\n",
    "# Visualize portfolio performance\n",
    "fig = go.Figure()\n",
    "\n",
    "# Our strategy\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['date'],\n",
    "        y=portfolio_values,\n",
    "        mode='lines',\n",
    "        name='ML Strategy',\n",
    "        line=dict(color='#4a9b7f', width=3)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Buy and hold\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=results_df['date'],\n",
    "        y=buy_hold_values,\n",
    "        mode='lines',\n",
    "        name='Buy & Hold',\n",
    "        line=dict(color='#c0504d', width=2, dash='dash')\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add trade markers\n",
    "buy_trades = [t for t in trade_history if t['action'] == 'BUY']\n",
    "sell_trades = [t for t in trade_history if t['action'] == 'SELL']\n",
    "\n",
    "if buy_trades:\n",
    "    buy_dates = [t['date'] for t in buy_trades]\n",
    "    buy_values = [portfolio_values[results_df[results_df['date'] == d].index[0]] \n",
    "                  for d in buy_dates if d in results_df['date'].values]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=buy_dates[:len(buy_values)],\n",
    "            y=buy_values,\n",
    "            mode='markers',\n",
    "            name='Buy',\n",
    "            marker=dict(color='green', size=10, symbol='triangle-up')\n",
    "        )\n",
    "    )\n",
    "\n",
    "if sell_trades:\n",
    "    sell_dates = [t['date'] for t in sell_trades]\n",
    "    sell_values = [portfolio_values[results_df[results_df['date'] == d].index[0]] \n",
    "                   for d in sell_dates if d in results_df['date'].values]\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=sell_dates[:len(sell_values)],\n",
    "            y=sell_values,\n",
    "            mode='markers',\n",
    "            name='Sell',\n",
    "            marker=dict(color='red', size=10, symbol='triangle-down')\n",
    "        )\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Portfolio Value Over Time - ML Strategy vs Buy & Hold\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Portfolio Value ($)\",\n",
    "    height=500,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "So what did I learn from all this?\n",
    "\n",
    "1. **Sentiment matters, but not as much as you'd think.** Price action still dominates.\n",
    "\n",
    "2. **Feature engineering is everything.** The right features matter more than the fanciest models.\n",
    "\n",
    "3. **Ensemble methods work.** Different models catch different patterns.\n",
    "\n",
    "4. **Time series is tricky.** Never use random splits. Always validate temporally.\n",
    "\n",
    "5. **Transaction costs kill strategies.** What looks good in backtesting often fails in reality.\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "\n",
    "- Add more sophisticated NLP (BERT, FinBERT)\n",
    "- Include options flow data\n",
    "- Add regime detection (bull vs bear markets need different models)\n",
    "- Implement proper position sizing and risk management\n",
    "- Test on out-of-sample data from different time periods\n",
    "\n",
    "Remember: This is a demonstration system. Real trading requires much more robust testing, risk management, and probably a good lawyer. But the concepts here? They're solid. And that's what matters for a portfolio project.\n",
    "\n",
    "Thanks for following along. Now go build something awesome.\n",
    "\n",
    "- Cazandra Aporbo, MS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
